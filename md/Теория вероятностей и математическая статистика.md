# Теория вероятностей и математическая статистика

## 1. Случайный эксперимент и случайные события. σ-алгебра событий

Случайный эксперимент (испытание) - это эксперимент, результат которого нельзя предсказать заранее с полной определенностью даже при соблюдении одинаковых условий проведения. Примерами могут служить бросание игральной кости, измерение времени обслуживания клиента или определение срока службы электрической лампочки.

### Основные понятия:

- **Элементарное событие (ω)** - неразложимый результат эксперимента, который нельзя представить как совокупность других более простых исходов. Например, при бросании игральной кости выпадение конкретного числа от 1 до 6.
- **Пространство элементарных событий (Ω)** - множество всех возможных элементарных событий данного эксперимента. Например, Ω = {1, 2, 3, 4, 5, 6} при бросании игральной кости.
- **Случайное событие (A)** - подмножество пространства элементарных событий (A ⊆ Ω), некоторое условие, которое может выполниться или не выполниться в результате эксперимента. Например, "выпадение четного числа" = {2, 4, 6}.

### Операции над событиями:

- **Сумма (объединение) событий A ∪ B** - событие, состоящее в наступлении хотя бы одного из событий A или B.
- **Произведение (пересечение) событий A ∩ B** - событие, состоящее в наступлении обоих событий A и B одновременно.
- **Разность событий A \ B** - событие, состоящее в наступлении события A и ненаступлении события B.
- **Противоположное событие Ā** - событие, состоящее в ненаступлении события A.

### σ-алгебра событий:

Система подмножеств F пространства Ω является σ-алгеброй, если выполняются следующие условия:
1. Пустое множество принадлежит F: ∅ ∈ F
2. С любым событием A система содержит и противоположное ему: если A ∈ F, то A̅ ∈ F
3. Система замкнута относительно счетного объединения: если A₁, A₂, ... ∈ F, то ⋃Aₙ ∈ F

Из этих условий также следует, что:
- Все пространство Ω принадлежит F: Ω ∈ F
- Система замкнута относительно счетного пересечения: если A₁, A₂, ... ∈ F, то ⋂Aₙ ∈ F

Понятие σ-алгебры имеет фундаментальное значение для аксиоматического построения теории вероятностей, так как именно на σ-алгебре определяется вероятностная мера.

### Аксиоматическое определение:

Вероятностным пространством называется тройка (Ω, F, P), где:
- Ω - пространство элементарных событий
- F - σ-алгебра подмножеств Ω (алгебра событий)
- P - вероятностная мера на F

Функция P, определенная на σ-алгебре F, называется вероятностью, если выполняются следующие аксиомы Колмогорова:
1. **Неотрицательность**: P(A) ≥ 0 для любого A ∈ F
2. **Нормированность**: P(Ω) = 1
3. **Счетная аддитивность**: для любой последовательности попарно несовместных событий A₁, A₂, ... из F имеем P(⋃Aₙ) = ∑P(Aₙ)

### Свойства вероятности:

1. Вероятность невозможного события равна нулю: P(∅) = 0
2. Если A ⊆ B, то P(A) ≤ P(B) (монотонность вероятности)
3. Вероятность противоположного события: P(A̅) = 1 - P(A)
4. Формула сложения вероятностей для двух событий: P(A ∪ B) = P(A) + P(B) - P(A ∩ B)
5. Вероятность события находится в интервале от 0 до 1: 0 ≤ P(A) ≤ 1 для любого A ∈ F
6. Если события A₁, A₂, ..., Aₙ попарно несовместны, то P(A₁ ∪ A₂ ∪ ... ∪ Aₙ) = P(A₁) + P(A₂) + ... + P(Aₙ)
7. Формула включения-исключения: P(A₁ ∪ A₂ ∪ ... ∪ Aₙ) = ∑P(Aᵢ) - ∑P(Aᵢ ∩ Aⱼ) + ∑P(Aᵢ ∩ Aⱼ ∩ Aₖ) - ... + (-1)ⁿ⁻¹P(A₁ ∩ A₂ ∩ ... ∩ Aₙ)

### Классическая и геометрическая вероятности:

**Классическая вероятность** (для конечного числа равновозможных исходов):
P(A) = m/n, где m - число благоприятных исходов, n - общее число исходов

Этот подход применим, когда:
- Число элементарных исходов конечно
- Все исходы равновозможны
- Каждый исход можно однозначно классифицировать как благоприятный или неблагоприятный

**Геометрическая вероятность** (для непрерывного случая):
P(A) = μ(A)/μ(Ω), где μ - мера множества (длина, площадь, объем и т.д.)

Применяется, когда элементарные исходы можно отождествить с точками некоторого геометрического пространства.

## 2. Условная вероятность и независимость событий

### Условная вероятность:

Вероятность события A при условии, что произошло событие B (при P(B) > 0):
P(A|B) = P(A ∩ B) / P(B)

Условная вероятность выражает "переоценку" вероятности события A после получения дополнительной информации о том, что произошло событие B.

### Свойства условной вероятности:

1. P(A|B) ≥ 0 для любого события A
2. P(Ω|B) = 1
3. Если A₁, A₂, ... - попарно несовместные события, то P(⋃Aₙ|B) = ∑P(Aₙ|B)
4. P(A₁ ∩ A₂|B) = P(A₁|B) × P(A₂|A₁ ∩ B)

### Независимость событий:

События A и B называются независимыми, если:
P(A ∩ B) = P(A) × P(B)

Эквивалентные определения:
- P(A|B) = P(A) (при P(B) > 0)
- P(B|A) = P(B) (при P(A) > 0)

Интуитивно, события независимы, если наступление одного из них не меняет вероятность наступления другого.

Понятие независимости может быть обобщено на произвольное число событий:
- События A₁, A₂, ..., Aₙ называются попарно независимыми, если для любых i ≠ j: P(Aᵢ ∩ Aⱼ) = P(Aᵢ) × P(Aⱼ)
- События A₁, A₂, ..., Aₙ называются независимыми в совокупности, если для любого подмножества индексов {i₁, i₂, ..., iₖ}: P(Aᵢ₁ ∩ Aᵢ₂ ∩ ... ∩ Aᵢₖ) = P(Aᵢ₁) × P(Aᵢ₂) × ... × P(Aᵢₖ)

### Формулы сложения, полной вероятности и Байеса:

**Формула сложения вероятностей:**
P(A ∪ B) = P(A) + P(B) - P(A ∩ B)

**Формула полной вероятности:**
P(A) = ∑P(A|Hₙ) × P(Hₙ), где H₁, H₂, ... - полная группа несовместных событий (т.е. Hᵢ ∩ Hⱼ = ∅ при i ≠ j и ⋃Hₙ = Ω)

**Формула Байеса:**
P(Hᵢ|A) = [P(A|Hᵢ) × P(Hᵢ)] / ∑[P(A|Hₙ) × P(Hₙ)]

Формула Байеса позволяет "переоценить" вероятности гипотез Hᵢ после наблюдения события A. Исходные вероятности P(Hᵢ) называются априорными, а вероятности P(Hᵢ|A) - апостериорными.

## 3. Схема Бернулли и предельные теоремы

### Схема Бернулли:

Последовательность независимых испытаний, в каждом из которых вероятность успеха равна p, а неудачи q = 1-p, называется схемой Бернулли. Эта схема моделирует ситуации, когда:
- Проводится фиксированное число n одинаковых испытаний
- Испытания независимы друг от друга
- В каждом испытании возможны только два исхода: "успех" и "неудача"
- Вероятность успеха p постоянна во всех испытаниях

Вероятность k успехов в n испытаниях вычисляется по формуле Бернулли:
P₍ₙ₎(k) = C(n,k) × pᵏ × q^(n-k), где C(n,k) = n!/(k!(n-k)!) - число сочетаний из n по k

### Предельные теоремы:

**Локальная теорема Муавра-Лапласа:**
При n → ∞ и фиксированном p (0 < p < 1):
P₍ₙ₎(k) ≈ φ(x) / √(npq), где x = (k-np)/√(npq), φ(x) = exp(-x²/2)/√(2π)

Эта теорема аппроксимирует вероятность ровно k успехов при больших n с помощью плотности нормального распределения.

**Интегральная теорема Муавра-Лапласа:**
P₍ₙ₎(a ≤ k ≤ b) ≈ Φ(β) - Φ(α), где 
- α = (a-0.5-np)/√(npq)
- β = (b+0.5-np)/√(npq)
- Φ(x) = ∫φ(t)dt от -∞ до x - функция Лапласа

Эта теорема аппроксимирует вероятность того, что число успехов лежит в интервале [a,b], с помощью функции нормального распределения.

**Теорема Пуассона:**
При n → ∞, p → 0, λ = np = const:
P₍ₙ₎(k) ≈ e^(-λ) × λᵏ/k!

Эта теорема применяется, когда число испытаний очень велико, а вероятность успеха в каждом испытании очень мала, при этом их произведение λ = np остается постоянным. Теорема Пуассона хорошо подходит для моделирования редких событий, таких как число вызовов в call-центр, число аварий на участке дороги и т.п.

Распределение Пуассона имеет математическое ожидание и дисперсию, равные λ, что упрощает его практическое применение.

## 4. Случайные величины (СВ). Свойства функции распределения (ФР).

### Случайные величины

Случайная величина (СВ) — это функция X(ω), определенная на пространстве элементарных событий Ω и принимающая числовые значения. По сути, случайная величина сопоставляет каждому исходу эксперимента некоторое число.

Формально, случайной величиной называется измеримая функция X: Ω → ℝ, то есть такая, что для любого борелевского множества B ⊂ ℝ множество {ω ∈ Ω: X(ω) ∈ B} принадлежит σ-алгебре F.

Случайные величины классифицируются на:
- **Дискретные** — принимают конечное или счетное множество значений
- **Непрерывные** — принимают значения из некоторого интервала
- **Смешанные** — сочетают свойства дискретных и непрерывных

### Функция распределения случайной величины

Функция распределения (ФР) случайной величины X определяется как:
F(x) = P(X ≤ x) для всех x ∈ ℝ

Функция распределения полностью характеризует вероятностные свойства случайной величины.

### Свойства функции распределения:

1. **Монотонность**: если x₁ < x₂, то F(x₁) ≤ F(x₂)
2. **Ограниченность**: 0 ≤ F(x) ≤ 1 для всех x
3. **Нормированность**: lim(x→-∞) F(x) = 0, lim(x→+∞) F(x) = 1
4. **Непрерывность справа**: lim(h→0+) F(x+h) = F(x)
5. **Вероятность попадания в интервал**: P(a < X ≤ b) = F(b) - F(a)
6. **Вероятность конкретного значения**: P(X = a) = F(a) - F(a-0), где F(a-0) = lim(h→0+) F(a-h)

## 5. Дискретные СВ: определение, построение функции распределения, примеры основных распределений

### Определение дискретной случайной величины

Случайная величина X называется дискретной, если она принимает конечное или счетное множество значений {x₁, x₂, ..., xₙ, ...} с вероятностями p₁, p₂, ..., pₙ, ..., где pᵢ = P(X = xᵢ) и ∑pᵢ = 1.

Дискретная СВ полностью задается своим законом распределения — совокупностью возможных значений и соответствующих им вероятностей:

| X    | x₁   | x₂   | ... | xₙ   | ... |
|------|------|------|-----|------|-----|
| P(X) | p₁   | p₂   | ... | pₙ   | ... |

### Построение функции распределения дискретной СВ

Функция распределения дискретной СВ имеет вид:
F(x) = ∑(pᵢ) для всех xᵢ ≤ x

Графически ФР дискретной СВ представляет собой ступенчатую функцию, имеющую скачки величиной pᵢ в точках xᵢ.

### Основные дискретные распределения:

1. **Распределение Бернулли** (принимает значения 0 и 1):
   - P(X = 1) = p, P(X = 0) = 1-p
   - Математическое ожидание: E(X) = p
   - Дисперсия: D(X) = p(1-p)
   - Применение: моделирование одиночного испытания с двумя исходами

2. **Биномиальное распределение** (число успехов в n независимых испытаниях):
   - P(X = k) = C(n,k) × pᵏ × (1-p)^(n-k), k = 0,1,...,n
   - Математическое ожидание: E(X) = np
   - Дисперсия: D(X) = np(1-p)
   - Применение: моделирование числа успехов в схеме Бернулли

3. **Геометрическое распределение** (число испытаний до первого успеха):
   - P(X = k) = (1-p)^(k-1) × p, k = 1,2,...
   - Математическое ожидание: E(X) = 1/p
   - Дисперсия: D(X) = (1-p)/p²
   - Применение: моделирование ожидания редких событий

4. **Распределение Пуассона** (число событий за фиксированный интервал):
   - P(X = k) = (λᵏ × e^(-λ))/k!, k = 0,1,2,...
   - Математическое ожидание: E(X) = λ
   - Дисперсия: D(X) = λ
   - Применение: моделирование редких событий (число звонков в колл-центр, число дефектов в материале)

## 6. Непрерывные СВ (определение и примеры основных распределений), свойства плотности распределения непрерывных СВ

### Определение непрерывной случайной величины

Случайная величина X называется непрерывной, если её функция распределения F(x) является непрерывной функцией и существует неотрицательная функция f(x) (плотность распределения), такая что:

F(x) = ∫f(t)dt от -∞ до x

Важное свойство непрерывных СВ: P(X = a) = 0 для любого значения a.

### Свойства плотности распределения:

1. **Неотрицательность**: f(x) ≥ 0 для всех x
2. **Нормированность**: ∫f(x)dx от -∞ до +∞ = 1
3. **Связь с функцией распределения**: f(x) = F'(x) (в точках, где F(x) дифференцируема)
4. **Вероятность попадания в интервал**: P(a ≤ X ≤ b) = ∫f(x)dx от a до b

### Основные непрерывные распределения:

1. **Равномерное распределение** на отрезке [a,b]:
   - Плотность: f(x) = 1/(b-a) при a ≤ x ≤ b, f(x) = 0 вне [a,b]
   - Функция распределения: F(x) = 0 при x < a, F(x) = (x-a)/(b-a) при a ≤ x ≤ b, F(x) = 1 при x > b
   - Математическое ожидание: E(X) = (a+b)/2
   - Дисперсия: D(X) = (b-a)²/12
   - Применение: моделирование случайных величин с равновероятными значениями

2. **Нормальное распределение** (распределение Гаусса):
   - Плотность: f(x) = (1/(σ√(2π))) × exp(-(x-μ)²/(2σ²))
   - Функция распределения: F(x) = (1/2) × [1 + erf((x-μ)/(σ√2))]
   - Математическое ожидание: E(X) = μ
   - Дисперсия: D(X) = σ²
   - Применение: моделирование суммы большого числа независимых случайных величин

3. **Показательное распределение**:
   - Плотность: f(x) = λe^(-λx) при x ≥ 0, f(x) = 0 при x < 0
   - Функция распределения: F(x) = 0 при x < 0, F(x) = 1-e^(-λx) при x ≥ 0
   - Математическое ожидание: E(X) = 1/λ
   - Дисперсия: D(X) = 1/λ²
   - Применение: моделирование времени между событиями в пуассоновском потоке

## 7. Многомерные СВ – определение. ФР – определение и свойства.

### Многомерные случайные величины

Многомерной случайной величиной (или случайным вектором) называется упорядоченный набор случайных величин (X₁, X₂, ..., Xₙ), определенных на одном вероятностном пространстве.

С геометрической точки зрения, многомерная случайная величина представляет собой случайную точку в n-мерном пространстве, координаты которой являются случайными величинами.

### Функция распределения многомерной случайной величины

Функцией распределения n-мерной случайной величины (X₁, X₂, ..., Xₙ) называется функция F(x₁, x₂, ..., xₙ), определяющая вероятность того, что все компоненты случайного вектора одновременно не превосходят соответствующих значений:

F(x₁, x₂, ..., xₙ) = P(X₁ ≤ x₁, X₂ ≤ x₂, ..., Xₙ ≤ xₙ)

### Свойства функции распределения многомерной случайной величины:

1. **Ограниченность**: 0 ≤ F(x₁, x₂, ..., xₙ) ≤ 1

2. **Монотонность**: функция F не убывает по каждому аргументу, т.е. если xᵢ ≤ yᵢ для всех i = 1,2,...,n, то F(x₁, x₂, ..., xₙ) ≤ F(y₁, y₂, ..., yₙ)

3. **Предельные свойства**:
   - Если хотя бы один из аргументов стремится к минус бесконечности, то F → 0
   - Если все аргументы стремятся к плюс бесконечности, то F → 1

4. **Непрерывность справа**: функция F непрерывна справа по каждому аргументу

5. **Вероятность попадания в прямоугольник**: вероятность попадания в прямоугольник (a₁,b₁] × (a₂,b₂] × ... × (aₙ,bₙ] равна:
   
   P(a₁ < X₁ ≤ b₁, a₂ < X₂ ≤ b₂, ..., aₙ < Xₙ ≤ bₙ) = 
   = ∑ (-1)^s F(c₁, c₂, ..., cₙ)
   
   где суммирование ведется по всем наборам (c₁, c₂, ..., cₙ), где cᵢ равно либо aᵢ, либо bᵢ, а s — число компонент, равных aᵢ

6. **Связь с одномерными функциями распределения**: одномерные функции распределения компонент можно получить как предельные значения совместной функции распределения:
   
   Fᵢ(xᵢ) = F(∞, ..., ∞, xᵢ, ∞, ..., ∞)

## 8. Дискретные многомерные СВ – определение, способ задания, вывод распределений одномерных случайных величин

### Дискретные многомерные случайные величины

Многомерная случайная величина (X₁, X₂, ..., Xₙ) называется дискретной, если все её компоненты X₁, X₂, ..., Xₙ являются дискретными случайными величинами, то есть каждая из них принимает значения из конечного или счетного множества с определенными вероятностями.

### Способ задания дискретной многомерной СВ

Дискретная многомерная СВ задается с помощью совместного закона распределения — таблицы, содержащей все возможные комбинации значений компонент и соответствующие им вероятности:

P(X₁ = x₁ₖ, X₂ = x₂ₗ, ..., Xₙ = xₙₘ) = pₖₗ...ₘ

где сумма всех вероятностей pₖₗ...ₘ равна 1.

Для двумерной случайной величины (X, Y) закон распределения можно представить в виде таблицы:

| X\Y | y₁   | y₂   | ... | yₘ   |
|-----|------|------|-----|------|
| x₁  | p₁₁  | p₁₂  | ... | p₁ₘ  |
| x₂  | p₂₁  | p₂₂  | ... | p₂ₘ  |
| ... | ...  | ...  | ... | ...  |
| xₙ  | pₙ₁  | pₙ₂  | ... | pₙₘ  |

### Вывод распределений одномерных случайных величин

Зная совместное распределение многомерной дискретной СВ (X₁, X₂, ..., Xₙ), можно найти распределение любой её компоненты Xᵢ, используя формулу:

P(Xᵢ = xᵢₖ) = ∑ P(X₁ = x₁ⱼ₁, X₂ = x₂ⱼ₂, ..., Xᵢ = xᵢₖ, ..., Xₙ = xₙⱼₙ)

где суммирование ведется по всем возможным наборам значений j₁, j₂, ..., jᵢ₋₁, jᵢ₊₁, ..., jₙ.

Для двумерной случайной величины (X, Y):
- P(X = xᵢ) = ∑ₖ P(X = xᵢ, Y = yₖ) — суммирование по строке
- P(Y = yⱼ) = ∑ₖ P(X = xₖ, Y = yⱼ) — суммирование по столбцу

### Условные распределения и независимость

**Условное распределение** компоненты Xᵢ при условии, что остальные компоненты приняли определенные значения:

P(Xᵢ = xᵢₖ | X₁ = x₁ⱼ₁, ..., Xᵢ₋₁ = xᵢ₋₁ⱼᵢ₋₁, Xᵢ₊₁ = xᵢ₊₁ⱼᵢ₊₁, ..., Xₙ = xₙⱼₙ) = 
= P(X₁ = x₁ⱼ₁, ..., Xᵢ₋₁ = xᵢ₋₁ⱼᵢ₋₁, Xᵢ = xᵢₖ, Xᵢ₊₁ = xᵢ₊₁ⱼᵢ₊₁, ..., Xₙ = xₙⱼₙ) / P(X₁ = x₁ⱼ₁, ..., Xᵢ₋₁ = xᵢ₋₁ⱼᵢ₋₁, Xᵢ₊₁ = xᵢ₊₁ⱼᵢ₊₁, ..., Xₙ = xₙⱼₙ)

Случайные величины X₁, X₂, ..., Xₙ называются **независимыми**, если для любых значений x₁, x₂, ..., xₙ выполняется:

P(X₁ = x₁, X₂ = x₂, ..., Xₙ = xₙ) = P(X₁ = x₁) × P(X₂ = x₂) × ... × P(Xₙ = xₙ)

Для двумерной случайной величины (X, Y) независимость означает, что:
P(X = xᵢ, Y = yⱼ) = P(X = xᵢ) × P(Y = yⱼ) для всех i, j.

### Функции дискретной многомерной СВ (одномерный и двумерный случай)

Если (X₁, X₂, ..., Xₙ) — дискретная многомерная СВ и Z = g(X₁, X₂, ..., Xₙ), то Z также является случайной величиной.

Для нахождения распределения Z:
1. Определить множество всех возможных значений Z
2. Для каждого значения z найти вероятность P(Z = z) = P(g(X₁, X₂, ..., Xₙ) = z)

Для двумерной СВ (X, Y) и Z = g(X, Y):
P(Z = z) = ∑ P(X = xᵢ, Y = yⱼ), где суммирование ведется по всем парам (xᵢ, yⱼ), для которых g(xᵢ, yⱼ) = z.

## 9. Непрерывные многомерные СВ – определение, свойства плотности распределения

### Непрерывные многомерные случайные величины

Многомерная случайная величина (X₁, X₂, ..., Xₙ) называется непрерывной, если существует неотрицательная функция f(x₁, x₂, ..., xₙ) (плотность распределения), такая что функция распределения представима в виде:

F(x₁, x₂, ..., xₙ) = ∫₋∞^x₁ ∫₋∞^x₂ ... ∫₋∞^xₙ f(t₁, t₂, ..., tₙ) dt₁ dt₂ ... dtₙ

### Свойства плотности распределения:

1. **Неотрицательность**: f(x₁, x₂, ..., xₙ) ≥ 0 для всех (x₁, x₂, ..., xₙ)

2. **Нормированность**: ∫₋∞^∞ ∫₋∞^∞ ... ∫₋∞^∞ f(x₁, x₂, ..., xₙ) dx₁ dx₂ ... dxₙ = 1

3. **Связь с функцией распределения**: f(x₁, x₂, ..., xₙ) = ∂ⁿF(x₁, x₂, ..., xₙ)/∂x₁∂x₂...∂xₙ (в точках, где F дифференцируема)

4. **Вероятность попадания в область**: P((X₁, X₂, ..., Xₙ) ∈ D) = ∫∫...∫_D f(x₁, x₂, ..., xₙ) dx₁ dx₂ ... dxₙ

### Вывод распределений одномерных случайных величин

Плотность распределения компоненты Xᵢ (маргинальная плотность) можно найти как:

fᵢ(xᵢ) = ∫₋∞^∞ ... ∫₋∞^∞ f(x₁, ..., xᵢ₋₁, xᵢ, xᵢ₊₁, ..., xₙ) dx₁ ... dxᵢ₋₁ dxᵢ₊₁ ... dxₙ

Для двумерной случайной величины (X, Y) с плотностью f(x, y):
- Плотность распределения X: fₓ(x) = ∫₋∞^∞ f(x, y) dy
- Плотность распределения Y: fᵧ(y) = ∫₋∞^∞ f(x, y) dx

### Условные распределения и независимость

**Условная плотность распределения** компоненты Xᵢ при условии, что остальные компоненты приняли определенные значения:

f(xᵢ | x₁, ..., xᵢ₋₁, xᵢ₊₁, ..., xₙ) = f(x₁, ..., xₙ) / f(x₁, ..., xᵢ₋₁, xᵢ₊₁, ..., xₙ)

где f(x₁, ..., xᵢ₋₁, xᵢ₊₁, ..., xₙ) — совместная плотность всех компонент кроме Xᵢ.

Случайные величины X₁, X₂, ..., Xₙ называются **независимыми**, если их совместная плотность распределения представима в виде произведения одномерных плотностей:

f(x₁, x₂, ..., xₙ) = f₁(x₁) × f₂(x₂) × ... × fₙ(xₙ)

Для двумерной случайной величины (X, Y) независимость означает, что:
f(x, y) = fₓ(x) × fᵧ(y) для всех x, y.

### Функции непрерывной многомерной СВ (одномерный и двумерный случай)

Если (X₁, X₂, ..., Xₙ) — непрерывная многомерная СВ и Z = g(X₁, X₂, ..., Xₙ), то Z также является случайной величиной.

Для нахождения плотности распределения Z используются специальные методы, такие как:
- Метод функции распределения
- Метод характеристических функций
- Метод якобиана преобразования (для взаимно однозначных преобразований)

## 10. Формула свертки для многомерных непрерывных случайных величин

### Формула свертки для суммы независимых случайных величин

Если X и Y — независимые непрерывные случайные величины с плотностями распределения f_X(x) и f_Y(y) соответственно, то плотность распределения их суммы Z = X + Y определяется формулой свертки:

f_Z(z) = ∫_{-∞}^{∞} f_X(z-y) · f_Y(y) dy = ∫_{-∞}^{∞} f_X(x) · f_Y(z-x) dx

Этот результат можно обобщить на сумму n независимых случайных величин X₁, X₂, ..., Xₙ. Для Z = X₁ + X₂ + ... + Xₙ плотность распределения находится посредством последовательного применения формулы свертки.

### Свертка для линейных комбинаций

Для линейной комбинации Z = a₁X₁ + a₂X₂ + ... + aₙXₙ, где X₁, X₂, ..., Xₙ — независимые непрерывные случайные величины, а a₁, a₂, ..., aₙ — константы, плотность распределения Z вычисляется с помощью обобщенной формулы свертки.

### Примеры применения формулы свертки:

1. **Сумма двух равномерно распределенных случайных величин**:  
   Если X ~ U[0,a] и Y ~ U[0,b] — независимые случайные величины с равномерным распределением, то их сумма Z = X + Y имеет плотность распределения вида «трапеции»:

   f_Z(z) = {
     z/ab,                   если 0 ≤ z < min(a,b)
     min(a,b)/ab,            если min(a,b) ≤ z < max(a,b)
     (a+b-z)/ab,             если max(a,b) ≤ z < a+b
     0,                      во всех остальных случаях
   }

2. **Сумма двух независимых нормальных случайных величин**:  
   Если X ~ N(μ₁,σ₁²) и Y ~ N(μ₂,σ₂²), то Z = X + Y ~ N(μ₁+μ₂, σ₁²+σ₂²)

## 11. Определение и свойства математического ожидания, дисперсии. Моменты высших порядков.

### Математическое ожидание

**Математическое ожидание** (среднее значение) случайной величины X — это число, характеризующее среднее значение случайной величины.

Для дискретной случайной величины:
E(X) = ∑ x_i · P(X = x_i)

Для непрерывной случайной величины:
E(X) = ∫_{-∞}^{∞} x · f(x) dx

### Свойства математического ожидания:

1. **Линейность**: E(aX + bY) = a·E(X) + b·E(Y), где a и b — константы
2. **Константа**: E(c) = c, где c — константа
3. **Независимость**: Если X и Y независимы, то E(X·Y) = E(X)·E(Y)
4. **Монотонность**: Если X ≤ Y, то E(X) ≤ E(Y)
5. **Ограниченность**: Если a ≤ X ≤ b, то a ≤ E(X) ≤ b

### Дисперсия

**Дисперсия** случайной величины X — это мера разброса значений случайной величины вокруг её математического ожидания:

D(X) = Var(X) = E[(X - E(X))²] = E(X²) - [E(X)]²

Для дискретной случайной величины:
D(X) = ∑ [x_i - E(X)]² · P(X = x_i) = ∑ x_i² · P(X = x_i) - [E(X)]²

Для непрерывной случайной величины:
D(X) = ∫_{-∞}^{∞} [x - E(X)]² · f(x) dx = ∫_{-∞}^{∞} x² · f(x) dx - [E(X)]²

**Стандартное отклонение** σ_X = √D(X) имеет размерность, совпадающую с размерностью случайной величины.

### Свойства дисперсии:

1. **Неотрицательность**: D(X) ≥ 0
2. **Константа**: D(c) = 0, где c — константа
3. **Линейное преобразование**: D(aX + b) = a²·D(X), где a и b — константы
4. **Сумма независимых СВ**: Если X и Y независимы, то D(X+Y) = D(X) + D(Y)

### Моменты высших порядков

**Момент k-го порядка** случайной величины X относительно начала координат (начальный момент):
μ_k = E(X^k)

**Центральный момент k-го порядка** (относительно математического ожидания):
μ_k' = E[(X - E(X))^k]

Особое значение имеют:
- Третий центральный момент μ₃' = E[(X - E(X))³], характеризующий асимметрию распределения
- Четвертый центральный момент μ₄' = E[(X - E(X))⁴], характеризующий островершинность распределения

**Коэффициент асимметрии** (skewness):
γ₁ = μ₃' / σ³

**Коэффициент эксцесса** (kurtosis):
γ₂ = μ₄' / σ⁴ - 3

Коэффициент эксцесса показывает, насколько распределение «островершинно» по сравнению с нормальным распределением, для которого γ₂ = 0.

## 12. Многомерные СВ и их ФР. Дискретные и непрерывные многомерные СВ. Независимые СВ.

### Многомерные случайные величины и их функции распределения

**Многомерная случайная величина** (случайный вектор) — это упорядоченный набор случайных величин (X₁, X₂, ..., Xₙ), определенных на одном вероятностном пространстве.

**Функция распределения** n-мерной случайной величины:
F(x₁, x₂, ..., xₙ) = P(X₁ ≤ x₁, X₂ ≤ x₂, ..., Xₙ ≤ xₙ)

### Дискретные многомерные СВ

**Дискретная многомерная СВ** принимает значения из конечного или счетного множества точек n-мерного пространства.

Закон распределения дискретной многомерной СВ задается таблицей значений и соответствующих вероятностей:
P(X₁ = x₁ᵢ, X₂ = x₂ⱼ, ..., Xₙ = xₙₖ) = pᵢⱼ...ₖ

Для двумерной СВ (X,Y):
- **Одномерные распределения**: P(X = xᵢ) = ∑ⱼ pᵢⱼ, P(Y = yⱼ) = ∑ᵢ pᵢⱼ
- **Условные распределения**: P(X = xᵢ | Y = yⱼ) = pᵢⱼ / P(Y = yⱼ)

### Непрерывные многомерные СВ

**Непрерывная многомерная СВ** имеет плотность распределения f(x₁, x₂, ..., xₙ), такую что:
F(x₁, x₂, ..., xₙ) = ∫₋∞^x₁ ∫₋∞^x₂ ... ∫₋∞^xₙ f(t₁, t₂, ..., tₙ) dt₁ dt₂ ... dtₙ

Свойства плотности распределения многомерной СВ аналогичны свойствам одномерного случая.

Для двумерной СВ (X,Y):
- **Одномерные (маргинальные) плотности**: f_X(x) = ∫₋∞^∞ f(x,y) dy, f_Y(y) = ∫₋∞^∞ f(x,y) dx
- **Условные плотности**: f(x|y) = f(x,y) / f_Y(y), f(y|x) = f(x,y) / f_X(x)

### Независимые случайные величины

**Случайные величины X₁, X₂, ..., Xₙ называются независимыми**, если для любых значений x₁, x₂, ..., xₙ выполняется:

F(x₁, x₂, ..., xₙ) = F₁(x₁) · F₂(x₂) · ... · Fₙ(xₙ)

где Fᵢ(xᵢ) — функция распределения i-й компоненты.

Для дискретных СВ независимость означает:
P(X₁ = x₁, X₂ = x₂, ..., Xₙ = xₙ) = P(X₁ = x₁) · P(X₂ = x₂) · ... · P(Xₙ = xₙ)

Для непрерывных СВ независимость означает:
f(x₁, x₂, ..., xₙ) = f₁(x₁) · f₂(x₂) · ... · fₙ(xₙ)

## 13. Моменты многомерных СВ. Ковариация и коэффициент корреляции – определения и свойства.

### Моменты многомерных случайных величин

Для многомерной случайной величины (X₁, X₂, ..., Xₙ) определяются следующие типы моментов:

**Смешанный момент порядка (k₁ + k₂ + ... + kₙ)**:
E[X₁^k₁ · X₂^k₂ · ... · Xₙ^kₙ]

**Смешанный центральный момент**:
E[(X₁ - E(X₁))^k₁ · (X₂ - E(X₂))^k₂ · ... · (Xₙ - E(Xₙ))^kₙ]

### Ковариация

**Ковариация** случайных величин X и Y — это мера их совместной изменчивости:
Cov(X,Y) = E[(X - E(X))(Y - E(Y))] = E(XY) - E(X)E(Y)

Для дискретных СВ:
Cov(X,Y) = ∑ᵢ ∑ⱼ (xᵢ - E(X))(yⱼ - E(Y))P(X = xᵢ, Y = yⱼ)

Для непрерывных СВ:
Cov(X,Y) = ∫₋∞^∞ ∫₋∞^∞ (x - E(X))(y - E(Y))f(x,y) dx dy

### Свойства ковариации:

1. **Симметричность**: Cov(X,Y) = Cov(Y,X)
2. **Самоковариация**: Cov(X,X) = D(X)
3. **Линейность**: Cov(aX + bY, Z) = a·Cov(X,Z) + b·Cov(Y,Z), где a и b — константы
4. **Независимость**: Если X и Y независимы, то Cov(X,Y) = 0 (обратное неверно!)
5. **Дисперсия суммы**: D(X+Y) = D(X) + D(Y) + 2·Cov(X,Y)

### Коэффициент корреляции

**Коэффициент корреляции** (корреляция Пирсона) — это нормированная ковариация, характеризующая степень линейной зависимости между случайными величинами:

ρ(X,Y) = Cov(X,Y) / (σ_X · σ_Y) = Cov(X,Y) / √(D(X) · D(Y))

### Свойства коэффициента корреляции:

1. **Ограниченность**: -1 ≤ ρ(X,Y) ≤ 1
2. **Независимость**: Если X и Y независимы, то ρ(X,Y) = 0 (обратное неверно!)
3. **Линейная зависимость**:
   - ρ(X,Y) = 1, если Y = aX + b, a > 0 (прямая линейная зависимость)
   - ρ(X,Y) = -1, если Y = aX + b, a < 0 (обратная линейная зависимость)
4. **Инвариантность к линейным преобразованиям**: ρ(aX + b, cY + d) = sign(ac)·ρ(X,Y), где a, b, c, d — константы, ac ≠ 0

**Интерпретация коэффициента корреляции**:
- |ρ| < 0.3: слабая линейная связь
- 0.3 ≤ |ρ| < 0.7: умеренная линейная связь
- |ρ| ≥ 0.7: сильная линейная связь

Важно помнить, что коэффициент корреляции измеряет только линейную зависимость между случайными величинами. Отсутствие корреляции не означает независимость.

## 14. Определение и основные свойства характеристических функций (ХФ). ХФ основных распределений.

### Определение характеристической функции

**Характеристическая функция** случайной величины X определяется как математическое ожидание комплексной экспоненты:

φₓ(t) = E(e^(itX)) = ∫_{-∞}^{∞} e^(itx) f_X(x) dx

где i — мнимая единица, t — вещественный параметр.

Для дискретной СВ:
φₓ(t) = ∑_k e^(itx_k) P(X = x_k)

### Основные свойства характеристических функций:

1. **Нормированность**: φₓ(0) = 1
2. **Ограниченность**: |φₓ(t)| ≤ 1 для всех t
3. **Непрерывность**: φₓ(t) непрерывна по t
4. **Эрмитова симметрия**: φₓ(-t) = φₓ*(t), где φₓ* — комплексно-сопряженная функция
5. **Единственность**: Каждому распределению соответствует единственная ХФ и наоборот
6. **Связь с моментами**: Если существует E(X^n), то φₓ(t) n раз дифференцируема и
   φₓ^(n)(0) = i^n E(X^n)
7. **Независимость**: Если X и Y независимы, то φ_{X+Y}(t) = φₓ(t) · φᵧ(t)
8. **Линейное преобразование**: φ_{aX+b}(t) = e^(itb) · φₓ(at)

### Характеристические функции основных распределений:

1. **Дискретное равномерное распределение** на множестве {a, a+1, ..., b}:
   φₓ(t) = (e^(ita) - e^(it(b+1))) / ((b-a+1)(1-e^(it)))

2. **Непрерывное равномерное распределение** на отрезке [a,b]:
   φₓ(t) = (e^(itb) - e^(ita)) / (it(b-a))

3. **Биномиальное распределение** Bin(n,p):
   φₓ(t) = (pe^(it) + q)^n, где q = 1-p

4. **Распределение Пуассона** с параметром λ:
   φₓ(t) = exp(λ(e^(it) - 1))

5. **Нормальное распределение** N(μ,σ²):
   φₓ(t) = exp(iμt - σ²t²/2)

6. **Показательное распределение** с параметром λ:
   φₓ(t) = λ/(λ-it)

7. **Распределение Коши** с параметрами a и b:
   φₓ(t) = exp(iat - b|t|)

8. **Гамма-распределение** с параметрами α и λ:
   φₓ(t) = (1 - it/λ)^(-α)

## 15. Неравенство Чебышева и закон больших чисел. Центральная предельная теорема.

### Неравенство Чебышева

**Неравенство Чебышева** устанавливает верхнюю границу для вероятности того, что случайная величина отклонится от своего математического ожидания более чем на заданную величину:

P(|X - E(X)| ≥ ε) ≤ D(X)/ε² для любого ε > 0

Эквивалентная форма:
P(|X - E(X)| < ε) ≥ 1 - D(X)/ε²

Неравенство Чебышева показывает, что случайная величина с маленькой дисперсией сконцентрирована вблизи своего математического ожидания.

### Закон больших чисел

**Закон больших чисел** — фундаментальный результат теории вероятностей, утверждающий, что среднее арифметическое большого числа независимых и одинаково распределенных случайных величин стремится к их математическому ожиданию.

#### Теорема Хинчина (Слабый закон больших чисел):
Если X₁, X₂, ..., Xₙ — независимые одинаково распределенные случайные величины с математическим ожиданием E(Xᵢ) = μ и конечной дисперсией D(Xᵢ) = σ², то для любого ε > 0:

lim_{n→∞} P(|X̄ₙ - μ| < ε) = 1

где X̄ₙ = (X₁ + X₂ + ... + Xₙ)/n — выборочное среднее.

Это означает, что при увеличении объема выборки средняя арифметическая наблюдаемых значений сходится по вероятности к теоретическому среднему.

#### Теорема Колмогорова (Усиленный закон больших чисел):
Если X₁, X₂, ..., Xₙ, ... — независимые одинаково распределенные случайные величины с математическим ожиданием E(Xᵢ) = μ, то:

P(lim_{n→∞} X̄ₙ = μ) = 1

Это означает, что выборочное среднее сходится к математическому ожиданию с вероятностью 1 (почти наверное).

### Центральная предельная теорема

**Центральная предельная теорема (ЦПТ)** утверждает, что сумма большого числа независимых одинаково распределенных случайных величин имеет распределение, близкое к нормальному, независимо от распределения исходных величин.

#### Теорема Ляпунова:
Если X₁, X₂, ..., Xₙ — независимые одинаково распределенные случайные величины с математическим ожиданием E(Xᵢ) = μ и дисперсией D(Xᵢ) = σ², то при n → ∞:

(X₁ + X₂ + ... + Xₙ - nμ)/(σ√n) → N(0,1)

где сходимость понимается как сходимость по распределению.

В эквивалентной форме для выборочного среднего:
√n(X̄ₙ - μ)/σ → N(0,1)

Это означает, что распределение нормированной суммы (или выборочного среднего) при большом n приближается к стандартному нормальному распределению.

ЦПТ объясняет, почему нормальное распределение так часто встречается в природе — многие случайные величины можно представить как сумму большого числа малых независимых случайных факторов.

## 16. Основные понятия математической статистики: выборка, вариационный ряд, эмпирическая ФР, гистограмма и полигон частот. Выборочные моменты.

### Основные понятия математической статистики

**Математическая статистика** — раздел математики, занимающийся методами сбора, анализа и интерпретации эмпирических данных для выявления закономерностей случайных явлений.

**Генеральная совокупность** — множество всех объектов, относительно которых делаются выводы.

**Выборка** — подмножество элементов генеральной совокупности, отобранных для исследования.

**Случайная выборка** объема n — набор независимых одинаково распределенных случайных величин X₁, X₂, ..., Xₙ.

**Повторная выборка** — выборка, при которой отобранный элемент возвращается в генеральную совокупность.

**Бесповторная выборка** — выборка, при которой отобранный элемент не возвращается в генеральную совокупность.

### Вариационный ряд

**Вариационный ряд** — выборка, элементы которой упорядочены по возрастанию:
X₍₁₎ ≤ X₍₂₎ ≤ ... ≤ X₍ₙ₎

где X₍ᵢ₎ — i-я порядковая статистика.

**Статистический ряд** — таблица, содержащая различные значения признака и соответствующие им частоты (или относительные частоты).

### Эмпирическая функция распределения

**Эмпирическая функция распределения** (ЭФР) определяется как:
F̂ₙ(x) = n₍x₎/n

где n₍x₎ — число элементов выборки, меньших или равных x, а n — объем выборки.

Свойства ЭФР аналогичны свойствам теоретической функции распределения.

Теорема Гливенко-Кантелли утверждает, что ЭФР сходится к теоретической функции распределения F(x) равномерно по x при n → ∞, т.е.:
sup_{x∈ℝ} |F̂ₙ(x) - F(x)| → 0 по вероятности при n → ∞

### Гистограмма и полигон частот

**Гистограмма** — графическое представление распределения данных, состоящее из прямоугольников, площади которых пропорциональны частотам соответствующих интервалов.

Построение гистограммы:
1. Разбить диапазон данных на k интервалов одинаковой длины h
2. Подсчитать число наблюдений nᵢ, попадающих в каждый интервал
3. Высота столбца гистограммы над i-м интервалом равна nᵢ/(n·h)

При n → ∞ и h → 0 (так что n·h → ∞) гистограмма сходится к плотности распределения.

**Полигон частот** — ломаная линия, соединяющая точки, абсциссы которых равны значениям признака, а ординаты — соответствующим частотам или относительным частотам.

Для построения полигона частот:
1. По оси абсцисс откладываются значения признака
2. По оси ординат — соответствующие частоты или относительные частоты
3. Полученные точки соединяются отрезками прямых

### Выборочные моменты

**Выборочные моменты** — статистические оценки теоретических моментов распределения.

**Выборочное среднее** (оценка математического ожидания):
X̄ = (X₁ + X₂ + ... + Xₙ)/n

**Выборочная дисперсия**:
S² = (1/n) · ∑(Xᵢ - X̄)² = (1/n) · ∑Xᵢ² - X̄²

**Исправленная выборочная дисперсия** (несмещенная оценка):
S²ᵢₛₚᵣ = (n/(n-1)) · S² = (1/(n-1)) · ∑(Xᵢ - X̄)²

**Выборочное стандартное отклонение**:
S = √S²

**Исправленное выборочное стандартное отклонение**:
Sᵢₛₚᵣ = √S²ᵢₛₚᵣ

**Выборочные центральные моменты k-го порядка**:
m̂ₖ = (1/n) · ∑(Xᵢ - X̄)ᵏ

**Выборочный коэффициент асимметрии**:
g₁ = m̂₃ / (m̂₂)^(3/2)

**Выборочный коэффициент эксцесса**:
g₂ = m̂₄ / (m̂₂)² - 3

## 17. Классификация оценок. Эффективность оценок. Метод моментов Функция правдоподобия и оценки максимального правдоподобия.

### Классификация оценок

**Статистическая оценка** — функция от выборки, используемая для приближенного определения неизвестного параметра распределения.

**Точечная оценка** — оценка, представленная одним числом (например, выборочное среднее).

**Интервальная оценка** — оценка в виде интервала, содержащего истинное значение параметра с заданной вероятностью (например, доверительный интервал).

Свойства точечных оценок:

1. **Несмещенность**: оценка θ̂ параметра θ называется несмещенной, если E(θ̂) = θ. Если E(θ̂) ≠ θ, то оценка смещенная, а величина bias(θ̂) = E(θ̂) - θ называется смещением.

2. **Состоятельность**: оценка θ̂ называется состоятельной, если она сходится по вероятности к истинному значению параметра при увеличении объема выборки, т.е. θ̂ → θ при n → ∞.

3. **Асимптотическая нормальность**: оценка θ̂ асимптотически нормальна, если распределение √n(θ̂ - θ) стремится к нормальному распределению при n → ∞.

### Эффективность оценок

**Эффективность** — свойство оценки, характеризующее её точность по сравнению с другими несмещенными оценками.

Для несмещенных оценок эффективность определяется как отношение минимально возможной дисперсии к дисперсии данной оценки:

eff(θ̂) = D₀(θ̂) / D(θ̂)

где D₀(θ̂) — нижняя граница дисперсии любой несмещенной оценки (граница Крамера-Рао).

**Граница Крамера-Рао**: Дисперсия любой несмещенной оценки θ̂ не может быть меньше величины 1/I(θ), где I(θ) — информация Фишера:

I(θ) = E[(∂ln f(X,θ)/∂θ)²]

**Эффективная оценка** — несмещенная оценка, дисперсия которой равна границе Крамера-Рао.

### Метод моментов

**Метод моментов** — метод построения оценок, основанный на приравнивании выборочных моментов к теоретическим.

Процедура метода моментов:
1. Выразить теоретические моменты через неизвестные параметры
2. Приравнять теоретические моменты к соответствующим выборочным
3. Решить полученную систему уравнений относительно параметров

Например, для нормального распределения N(μ,σ²):
E(X) = μ и E(X²) = μ² + σ²

Приравнивая эти выражения к выборочным моментам, получаем оценки:
μ̂ = X̄ и σ̂² = (1/n)·∑(Xᵢ - X̄)²

### Функция правдоподобия и оценки максимального правдоподобия

**Функция правдоподобия** — функция, выражающая совместную плотность (или вероятность) выборки как функцию параметра:

L(θ) = f(X₁,X₂,...,Xₙ|θ)

Для независимых наблюдений:
L(θ) = ∏ᵢf(Xᵢ|θ)

**Логарифмическая функция правдоподобия**:
l(θ) = ln L(θ) = ∑ᵢln f(Xᵢ|θ)

**Метод максимального правдоподобия (ММП)** заключается в выборе оценки θ̂, максимизирующей функцию правдоподобия:

θ̂ = arg max L(θ) = arg max l(θ)

Для нахождения оценки максимального правдоподобия (ОМП) обычно решают уравнение правдоподобия:
∂l(θ)/∂θ = 0

**Свойства оценок максимального правдоподобия**:
1. Состоятельность
2. Асимптотическая нормальность
3. Асимптотическая эффективность
4. Инвариантность относительно параметризации

Преимущества ММП:
- Универсальность
- Хорошие асимптотические свойства
- Инвариантность

Недостатки ММП:
- Иногда сложно найти решение уравнения правдоподобия
- Оценки могут быть смещенными

## 18. Проверка статистических гипотез. Уровень значимости и мощность критерия. Ошибки 1-го и 2-го рода.

### Проверка статистических гипотез

**Статистическая гипотеза** — предположение о виде или параметрах распределения генеральной совокупности.

**Проверка статистической гипотезы** — процедура принятия решения о справедливости или несправедливости выдвинутой гипотезы на основе выборочных данных.

**Нулевая гипотеза (H₀)** — гипотеза, которая считается верной до тех пор, пока не будет доказано обратное.

**Альтернативная гипотеза (H₁)** — гипотеза, которая противопоставляется нулевой.

**Статистический критерий** — правило, по которому принимается решение о принятии или отклонении нулевой гипотезы.

**Критическая область** — множество значений статистики критерия, при которых нулевая гипотеза отклоняется.

**Область принятия гипотезы** — множество значений статистики критерия, при которых нулевая гипотеза не отклоняется.

### Ошибки 1-го и 2-го рода

**Ошибка 1-го рода** — отклонение нулевой гипотезы, когда она верна (ложное отклонение).

**Ошибка 2-го рода** — принятие нулевой гипотезы, когда она неверна (ложное принятие).

| Решение \ Реальность | H₀ верна | H₀ неверна |
|----------------------|----------|------------|
| Принять H₀           | Верное решение | Ошибка 2-го рода (β) |
| Отклонить H₀         | Ошибка 1-го рода (α) | Верное решение |

### Уровень значимости и мощность критерия

**Уровень значимости (α)** — вероятность совершить ошибку 1-го рода:
α = P(отклонить H₀ | H₀ верна)

Обычно используются стандартные уровни значимости: 0.01, 0.05, 0.1.

**Мощность критерия (1-β)** — вероятность отклонить нулевую гипотезу, когда она неверна:
1-β = P(отклонить H₀ | H₀ неверна)

где β — вероятность ошибки 2-го рода.

**p-значение** — наименьший уровень значимости, при котором нулевая гипотеза отклоняется для данной выборки:
- Если p-значение < α, то H₀ отклоняется
- Если p-значение ≥ α, то H₀ не отклоняется

### Общая схема проверки гипотез:

1. Формулировка нулевой и альтернативной гипотез
2. Выбор статистики критерия
3. Определение уровня значимости α
4. Определение критической области
5. Вычисление значения статистики критерия по выборке
6. Принятие решения: если значение статистики попадает в критическую область, H₀ отклоняется; в противном случае H₀ не отклоняется

## 19. Критерий отношения правдоподобия. Критерий согласия Пирсона.

### Критерий отношения правдоподобия

**Критерий отношения правдоподобия (КОП)** — универсальный метод построения статистических критериев, основанный на отношении функций правдоподобия при альтернативной и нулевой гипотезах.

**Отношение правдоподобия**:
λ = L(H₁) / L(H₀)

где L(H₀) и L(H₁) — значения функции правдоподобия при параметрах, соответствующих нулевой и альтернативной гипотезам.

Или в логарифмической форме:
ln λ = ln L(H₁) - ln L(H₀)

**Статистика критерия**:
Λ = -2 ln λ

Согласно теореме Вилкса, при справедливости H₀ и некоторых условиях регулярности, статистика Λ асимптотически (при n → ∞) имеет χ²-распределение с числом степеней свободы, равным разности размерностей параметрических пространств H₁ и H₀.

**Правило принятия решения**:
- Если Λ > χ²₍α,k₎, то H₀ отклоняется на уровне значимости α
- Если Λ ≤ χ²₍α,k₎, то H₀ не отклоняется

где χ²₍α,k₎ — критическое значение χ²-распределения с k степенями свободы.

### Критерий согласия Пирсона (χ²)

**Критерий согласия Пирсона (χ²)** — критерий для проверки гипотезы о соответствии эмпирического распределения теоретическому.

**Статистика критерия**:
χ² = ∑(nᵢ - npᵢ)² / (npᵢ)

где:
- nᵢ — фактические частоты в i-м интервале
- pᵢ — теоретические вероятности попадания в i-й интервал
- n — объем выборки

При справедливости нулевой гипотезы статистика χ² асимптотически имеет χ²-распределение с k-r-1 степенями свободы, где:
- k — число интервалов группировки
- r — число параметров распределения, оцененных по выборке

**Правило принятия решения**:
- Если χ² > χ²₍α,k-r-1₎, то H₀ отклоняется на уровне значимости α
- Если χ² ≤ χ²₍α,k-r-1₎, то H₀ не отклоняется

**Процедура применения критерия Пирсона**:
1. Разбить диапазон данных на k интервалов
2. Подсчитать фактические частоты nᵢ попадания в каждый интервал
3. Рассчитать теоретические вероятности pᵢ (на основе проверяемого распределения)
4. Рассчитать статистику χ²
5. Сравнить χ² с критическим значением и принять решение

**Рекомендации**:
- Число интервалов k должно быть не менее 7-8
- Теоретическая частота npᵢ в каждом интервале должна быть не менее 5
- При необходимости соседние интервалы объединяются

**Преимущества критерия χ²**:
- Универсальность
- Применимость к дискретным и непрерывным распределениям
- Возможность проверки сложных гипотез

**Недостатки критерия χ²**:
- Проверяет только приближенное соответствие
- Чувствительность к выбору интервалов
- Требует большого объема выборки